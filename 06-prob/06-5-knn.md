## K-Nearest Neighbors

* Motivational Idea: Things with the same class will have
  similar features
  
* Setup:

  1. Accumulate a "large" number of "representative"
     pre-classified examples: a "training set"

  2. Figure out what "features" of examples you think
     should matter

  3. Establish a *distance metric* between examples based on
     the difference in their features

* Classification:

  1. Calculate the distance between the target and each 
     example in the training set
     
  2. Find the *k* closest examples for some integer *k*
  
  3. Classify the target with the most common label from the
     *k* examples

## Issues With KNN

* Training set

* What distance metric to use

* How to choose *k*

* How to break ties

* Performance (!)

## Training Set Construction

* Larger is better for accuracy, but again there's a sqrt
  heuristic
  
* Training set should be drawn from a population of
  examples similar to that of the expected targets
  
* Getting classified instances is often expensive

## Distance Metric

* Usually some Cartesian metric: sum of squares of
  differences between feature values for a pair of examples
  
* For boolean features (common) this reduces to "Hamming
  Distance"
  
        001011
        011110
        ------
        010101 = 3

## Choosing k

* (From Bard, but checked by me)

* Rule of thumb: *k = sqrt(|T|)*

* Larger for weak data, smaller for *|T|* small

* Overfitting is a thing: check using techniques to be
  discussed later, especially training/test splits and
  cross-validation

## Tie-breaking

* Pick *k* based on the number of classes, such that no tie
  is possible: may require sequence of *k*s
  
  Example: three classes. Choose *k = 4*. At most two
  classes can tie. Break that tie with *k = 3*

* Break ties randomly

* Expand *k* until tie is broken

## Performance

* The whole training set has to be kept around for every
  classification. This can be big and unwieldy
  
  One approach is clustering: "summarizing" the data
  with less granularity
  
* Na√Øvely, classifying requires traversing the whole training
  set. This can be expensive
  
  There are data structures that can be built to improve the
  performance of the search

## Start With KNN

* KNN is a great starting point for learned
  classification
  
  * Often classifies accurately
  * Simple to build, understand and reason about

* When accuracy, performance etc are a problem,
  easy to move "up" to stronger methods

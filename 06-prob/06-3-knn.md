## The Classification Problem

* Given a thing with attributes, assign a class to the thing

* Examples:

  * Given the sides of a triangle, is it equilateral,
    scalene or isosceles?
  * Given an email message, is it ham or spam?
  * Given a book, which author wrote it?
  * Given a recording, which instrument is playing?

* This is a really fundamental AI problem…

* Closely associated with machine learning

## K-Nearest Neighbors

* Motivational Idea: Things with the same class will have
  similar features
  
* Setup:

  1. Accumulate a "large" number of "representative"
     pre-classified examples: a "training set"

  2. Establish a *distance metric* between examples based on
     the difference in their features

* Classification:

  1. Calculate the distance between the target and each 
     example in the training set
     
  2. Find the *k* closest examples for some integer *k*
  
  3. Classify the target with the most common label from the
     *k* examples

## Issues With KNN

* How to break ties

* How to choose *k*

* What distance metric to use

* Training set

* Performance (!)

## Tie-breaking

* Pick *k* based on the number of classes, such that no tie
  is possible: may require sequence of *k*s
  
  Example: three classes. Choose *k = 4*. At most two
  classes can tie. Break that tie with *k = 3*

* Break ties randomly

* Expand *k* until tie is broken

## Choosing k

* (From Bard, but checked by me)

* Rule of thumb: *k = sqrt(|T|)*

* Larger for weak data, smaller for *|T|* small

* Overfitting is a thing: check using techniques to be
  discussed later, especially training/test splits and
  cross-validation

## Distance Metric

* Usually some Cartesian metric: sum of difference between feature
  values for all features, squared
  
* For boolean features (common) this reduces to "Hamming
  Distance"
  
## Training Set Construction

* Larger is better for accuracy, but again there's a sqrt
  heuristic
  
* Training set should be drawn from a population of
  examples similar to that of the expected targets
  
* Getting classified instances is often expensive

## Performance

* The whole training set has to be kept around for every
  classification. This can be big and unwieldy
  
  One approach is clustering: "summarizing" the data
  with less granularity
  
* Naïvely, classifying requires traversing the whole training
  set. This can be expensive
  
  There are data structures that can be built to improve the
  performance of the search

## Start With KNN

* KNN is a great starting point for learned
  classification
  
  * Often classifies accurately
  * Simple to build, understand and reason about

* When accuracy, performance etc are a problem,
  easy to move "up" to stronger methods
